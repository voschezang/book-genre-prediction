{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import os\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/Tristan/books/src'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import config\n",
    "import tfidf2 as tfidf\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.listdir(config.dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_lex_dict(lexicon_file):\n",
    "        \"\"\"\n",
    "        Convert lexicon file to a dictionary\n",
    "        \"\"\"\n",
    "        lex_dict = {}\n",
    "        for line in lexicon_file.split('\\n'):\n",
    "            (word, measure) = line.strip().split('\\t')[0:2]\n",
    "            lex_dict[word] = float(measure)\n",
    "        return lex_dict\n",
    "    \n",
    "sent_dict = make_lex_dict(open('/Users/Tristan/books/src/' +'vader_lexicon.txt', 'r').read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis. Analysis is performed for each sentence and the sentiment scores kept in lists. Sentiment scores are calculated by averaging the sentiment scores for all sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def return_sentiment_scores(sentence):\n",
    "    # return just the sentiment scores\n",
    "    snt = analyser.polarity_scores(sentence)\n",
    "    return snt\n",
    "\n",
    "def sentiment_analysis(directory):\n",
    "    analyser = SentimentIntensityAnalyzer()                    \n",
    "    # returns the sentiment of every book in the directory\n",
    "    data = pd.read_csv(config.dataset_dir + 'output/final_data.csv', index_col=0)\n",
    "    print(len(data.index))\n",
    "#     max_amt = len(data.index) + 2\n",
    "#     print(data.index, len(os.listdir(directory)))\n",
    "    pos_list = []\n",
    "    neg_list = []\n",
    "    neu_list = []\n",
    "    comp_list = []\n",
    "    \n",
    "    # for every book\n",
    "    for filename in data['filename']:#[:max_amt]:\n",
    "        \n",
    "        sub_pos_list = []\n",
    "        sub_neg_list = []\n",
    "        sub_neu_list = []\n",
    "        sub_comp_list = []\n",
    "        \n",
    "        # if file is a textfile\n",
    "        if filename.endswith(\".txt\"):\n",
    "            text = open(os.path.join(directory, filename), 'r', errors='replace')\n",
    "            # for every line in the text\n",
    "            for line in text.readlines():\n",
    "                scores = return_sentiment_scores(line)\n",
    "                # save sentiment scores \n",
    "                sub_neg_list.append(scores['neg'])\n",
    "                sub_neu_list.append(scores['neu'])\n",
    "                sub_pos_list.append(scores['pos'])\n",
    "                sub_comp_list.append(scores['compound'])\n",
    "            \n",
    "            # then save average sentiment scores for each book\n",
    "            neg_list.append((sum(sub_neg_list) / float(len(sub_neg_list))))\n",
    "            pos_list.append((sum(sub_pos_list) / float(len(sub_pos_list))))\n",
    "            neu_list.append((sum(sub_neu_list) / float(len(sub_neu_list))))\n",
    "            comp_list.append((sum(sub_comp_list) / float(len(sub_comp_list))))\n",
    "            \n",
    "    # convert scores to pandas compatible list\n",
    "    neg = pd.Series(neg_list)\n",
    "    pos = pd.Series(pos_list)\n",
    "    neu = pd.Series(neu_list)\n",
    "    com = pd.Series(comp_list)\n",
    "\n",
    "    print(len(neg), len(pos), len(neu), len(com))\n",
    "    # fill the right columns with the right data\n",
    "    print(type(data),'type')\n",
    "    print(neg)\n",
    "    data['neg score'] = neg.values\n",
    "    data['pos score'] = pos.values\n",
    "    data['neu score'] = neu.values\n",
    "    data['comp score'] = com.values\n",
    "    data.to_csv(config.dataset_dir + 'output/final_data.csv')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyser = SentimentIntensityAnalyzer()                    \n",
    "sentiment_analysis(config.dataset_dir + 'bookdatabase/books/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to count the amount of positive and negative words as features. We also create a new file for each book with just the sentiment words. As a result, we will be able to do tfidf on these files later and create wordclouds per genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_sentiment_words(directory):\n",
    "    sent_words_list =[]\n",
    "    pos_list = []\n",
    "    neg_list = []\n",
    "    \n",
    "    data = pd.read_csv(config.dataset_dir + 'output/final_data.csv', index_col=0)\n",
    "\n",
    "    for filename in data['filename']:\n",
    "        sent_words_list =[]\n",
    "        pos_count = 0\n",
    "        neg_count = 0\n",
    "        \n",
    "        if filename.endswith(\".txt\"):\n",
    "            text = open(os.path.join(directory, filename), 'r', errors='replace')\n",
    "            sentiment_file = open(config.dataset_dir +'output/sentiment_word_texts/' + filename , 'w')\n",
    "\n",
    "            for line in text.readlines():\n",
    "                for word in line.split(\" \"):\n",
    "                    if word in sent_dict:\n",
    "                        if sent_dict[word] >= 0:\n",
    "                            pos_count += 1\n",
    "                            sent_words_list.append(word)\n",
    "                            sentiment_file.write(\"%s\" % word)\n",
    "                            sentiment_file.write(\" \")\n",
    "                        else:\n",
    "                            neg_count += 1\n",
    "                            sentiment_file.write(\"%s\" % word)\n",
    "                            sentiment_file.write(\" \")\n",
    "\n",
    "            pos_list.append(pos_count)\n",
    "            neg_list.append(neg_count)\n",
    "            \n",
    "    data['amt pos'] = pos_list \n",
    "    data['amt neg'] = neg_list\n",
    "    \n",
    "    data.to_csv(config.dataset_dir + 'output/final_data.csv')\n",
    "    return data\n",
    "\n",
    "count_sentiment_words(config.dataset_dir + 'bookdatabase/books/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_unique_genres():\n",
    "    genres_file = open(config.dataset_dir + 'unique_genres.txt', 'r')\n",
    "    return[genre.strip('\\n') for genre in genres_file.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n"
     ]
    }
   ],
   "source": [
    "def create_wordcloud(scores, genre):\n",
    "    \n",
    "    font_path = config.dataset_dir + 'Open_Sans_Condensed/OpenSansCondensed-Light.ttf'\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "\n",
    "    try:\n",
    "        w = WordCloud(stopwords = stopWords, background_color='white', min_font_size=14, font_path=font_path, width = 1000, height = 500,relative_scaling=1,normalize_plurals=False)\n",
    "        wordcloud = w.generate_from_frequencies(scores)\n",
    "        wordcloud.recolor(color_func=grey_color_func)\n",
    "\n",
    "        \n",
    "    except ZeroDivisionError:\n",
    "        print('shit')\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(15,8))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(config.dataset_dir + 'output/wordclouds/' + genre + '.png')\n",
    "    plt.close()\n",
    "    \n",
    "def grey_color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n",
    "    return \"hsl(0, 0%%, %d%%)\" % random.randint(10, 50)\n",
    "\n",
    "\n",
    "def tfidf_per_genre(plot_wc=False):\n",
    "    data = pd.read_csv(config.dataset_dir + 'final_data.csv')\n",
    "    genres_file = open(config.dataset_dir + 'unique_genres.txt', 'r')\n",
    "    genre_list = [genre.strip('\\n') for genre in genres_file.readlines()]\n",
    "    directory = config.dataset_dir + 'output/sentiment_word_texts/'\n",
    "    doc_list = []\n",
    "    \n",
    "    # create a list of lists containing all tokens contained in the text of a certain genre\n",
    "    for genre in genre_list:\n",
    "        book_list = []\n",
    "        genre = genre.replace('/', ' ')\n",
    "        books_of_genre = data.loc[data['genre'] == genre]\n",
    "        \n",
    "        for book in books_of_genre['filename']:\n",
    "            book_list.append(book)\n",
    "        \n",
    "        genre_document = tfidf.genre_document(book_list, directory)\n",
    "        doc_list.append(genre_document)\n",
    "\n",
    "#     create index\n",
    "    index = tfidf.create_index(genre_list, doc_list)\n",
    "    # create tf_matrix\n",
    "    tf_matrix = tfidf.create_tf_matrix(genre_list, doc_list)\n",
    "    \n",
    "    # create scores for each genre\n",
    "    for genre, document in zip(genre_list, doc_list):\n",
    "        genre = genre.replace('/', ' ')\n",
    "        score_dict = {}\n",
    "        document = set(document)\n",
    "        try:\n",
    "            \n",
    "                for term in document:\n",
    "\n",
    "                    score = tfidf.tfidf(term, genre, doc_list, index, tf_matrix)\n",
    "                    score_dict[term] = score\n",
    "\n",
    "                scores_file = open(config.dataset_dir +'output/top200_per_genre/' + genre + '.txt', 'w')\n",
    "\n",
    "                for w in sorted(score_dict, key=score_dict.get, reverse=True):\n",
    "\n",
    "                    scores_file.write('%s/n' % w)\n",
    "\n",
    "                scores_file.close()\n",
    "\n",
    "                print('success')\n",
    "\n",
    "                if plot_wc:\n",
    "                    font_path = config.dataset_dir + 'Open_Sans_Condensed/OpenSansCondensed-Light.ttf'\n",
    "                    create_wordcloud(score_dict, genre)\n",
    "\n",
    "        except ZeroDivisionError:\n",
    "            print('reaallly')\n",
    "            continue\n",
    "        except ValueError:\n",
    "            continue\n",
    "    \n",
    "\n",
    "tfidf_dict_per_genre = tfidf_per_genre(plot_wc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(tfidf_dict_per_genre.keys())[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(list(tfidf_dict_per_genre.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(tfidf_dict_per_genre['Diary and Novel']) # may differ per genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_words_per_genre = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample = tfidf_dict_per_genre['War']\n",
    "i = list(sample.keys())[-1]\n",
    "sample[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max(list(sample.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Generate labels file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas, os\n",
    "import data, config\n",
    "from utils import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "info = pandas.read_csv(config.dataset_dir + 'final_data.csv')\n",
    "book_list = os.listdir(config.dataset_dir + 'output/sentiment_word_texts')\n",
    "labels = data.extract_genres(info, book_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "io.save_dict_to_csv(config.dataset_dir, 'labels', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (oud)\n",
    "Choose to most important to be kept in the feature-vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import data, config, tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "directory = config.dataset_dir + 'output/sentiment_word_texts'\n",
    "book_list = os.listdir(directory)\n",
    "book_list = book_list[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = tfidf.create_index(directory, book_list)\n",
    "tf_matrix = tfidf.create_tf_matrix(directory, book_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_dict = tfidf.perform_tfidf(directory, book_list, index, tf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (optional) show the result\n",
    "w = WordCloud(background_color='white', width=900, height=500, \n",
    "                      max_words=1628,relative_scaling=1,normalize_plurals=False)\n",
    "wordcloud = w.generate_from_frequencies(tfidf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "# plt.savefig(config.dataset_dir + 'output/wordclouds/' + genre + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tfidf_dict_per_genre = wordcloud_per_genre()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
